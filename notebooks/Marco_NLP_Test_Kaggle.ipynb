{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb71d20",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2fa627",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelBinarizer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#from wordcloud import WordCloud\n",
    "#from wordcloud import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5365f1",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f294ff",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36419e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true = pd.read_csv(\"./raw_data/True.csv\")\n",
    "false = pd.read_csv(\"./raw_data/Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd3fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "false.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5300a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true['category'] = 1\n",
    "false['category'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ff103",
   "metadata": {},
   "outputs": [],
   "source": [
    "true.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "false.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7f454",
   "metadata": {},
   "source": [
    "## Merging Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([true,false]) #Merging the 2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeccf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc3f5b",
   "metadata": {},
   "source": [
    "## Checking for NAN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a82cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c24997",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title.count()#Counting Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd871853",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.subject.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef5d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,8))\n",
    "sns.set(style = \"whitegrid\",font_scale = 1.2)\n",
    "chart = sns.countplot(x = \"subject\", hue = \"category\" , data = df)\n",
    "chart.set_xticklabels(chart.get_xticklabels(),rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14216e73",
   "metadata": {},
   "source": [
    "## Transforming different columns in one column with the Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e390f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'] + \" \" + df['title']\n",
    "del df['title']\n",
    "del df['subject']\n",
    "del df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e787177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a581f0",
   "metadata": {},
   "source": [
    "# STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5e2506",
   "metadata": {},
   "source": [
    "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c28425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a83a4a",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "#Apply function on review column\n",
    "df['text']=df['text'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8ac3b",
   "metadata": {},
   "source": [
    "## Words Clouding ( Ilustration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a533cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#True News\n",
    "plt.figure(figsize = (20,20)) # Text that is not Fake\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.category == 1].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93921b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fake News\n",
    "plt.figure(figsize = (20,20)) # Text that is Fake\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.category == 0].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3f9846",
   "metadata": {},
   "source": [
    "## Words Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd2d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\n",
    "text_len=df[df['category']==1]['text'].str.len()\n",
    "ax1.hist(text_len,color='red')\n",
    "ax1.set_title('Original text')\n",
    "text_len=df[df['category']==0]['text'].str.len()\n",
    "ax2.hist(text_len,color='green')\n",
    "ax2.set_title('Fake text')\n",
    "fig.suptitle('Characters in texts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd15b0",
   "metadata": {},
   "source": [
    "The distribution of both seems to be a bit different. 2500 characters in text is the most common in original text category while around 5000 characters in text are most common in fake text category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c79f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\n",
    "text_len=df[df['category']==1]['text'].str.split().map(lambda x: len(x))\n",
    "ax1.hist(text_len,color='red')\n",
    "ax1.set_title('Original text')\n",
    "text_len=df[df['category']==0]['text'].str.split().map(lambda x: len(x))\n",
    "ax2.hist(text_len,color='green')\n",
    "ax2.set_title('Fake text')\n",
    "fig.suptitle('Words in texts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\n",
    "word=df[df['category']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\n",
    "ax1.set_title('Original text')\n",
    "word=df[df['category']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\n",
    "sns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\n",
    "ax2.set_title('Fake text')\n",
    "fig.suptitle('Average word length in each text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff84422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addin a list the main words of a text\n",
    "def get_corpus(text):\n",
    "    words = []\n",
    "    for i in text:\n",
    "        for j in i.split():\n",
    "            words.append(j.strip())\n",
    "    return words\n",
    "\n",
    "corpus = get_corpus(df.text)\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b20351a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Couting recurrencies of most frequent words\n",
    "from collections import Counter\n",
    "counter = Counter(corpus)\n",
    "most_common = counter.most_common(10)\n",
    "most_common = dict(most_common)\n",
    "most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10cce8",
   "metadata": {},
   "source": [
    "## Words Engineering and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52b624",
   "metadata": {},
   "source": [
    "Text Explaining what is CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def get_top_text_ngrams(corpus, n, g):\n",
    "    vec = CountVectorizer(ngram_range=(g, g)).fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1429699",
   "metadata": {},
   "source": [
    "Unigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac5326",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "most_common_uni = get_top_text_ngrams(df.text,10,1)\n",
    "most_common_uni = dict(most_common_uni)\n",
    "sns.barplot(x=list(most_common_uni.values()),y=list(most_common_uni.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0addf670",
   "metadata": {},
   "source": [
    "Bigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea9df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "most_common_bi = get_top_text_ngrams(df.text,10,2)\n",
    "most_common_bi = dict(most_common_bi)\n",
    "sns.barplot(x=list(most_common_bi.values()),y=list(most_common_bi.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2ed307",
   "metadata": {},
   "source": [
    "Trigram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f44d4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "most_common_tri = get_top_text_ngrams(df.text,10,3)\n",
    "most_common_tri = dict(most_common_tri)\n",
    "sns.barplot(x=list(most_common_tri.values()),y=list(most_common_tri.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9529e5",
   "metadata": {},
   "source": [
    "# Spliting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da633c",
   "metadata": {},
   "source": [
    "Splitting the data into 2 parts - training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d5dc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(df.text,df.category,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93a827",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392bb0f7",
   "metadata": {},
   "source": [
    "# Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f43348c",
   "metadata": {},
   "source": [
    "Tokenizing Text -> Repsesenting each word by a number\n",
    "\n",
    "Mapping of orginal word to number is preserved in word_index property of tokenizer\n",
    "\n",
    "Tokenized applies basic processing like changing it to lower case, explicitely setting that as False\n",
    "\n",
    "Lets keep all news to 300, add padding to news with less than 300 words and truncating long ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d5d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e89ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "X_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df1c97",
   "metadata": {},
   "source": [
    "# Introduction to GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b16c7b",
   "metadata": {},
   "source": [
    "GloVe method is built on an important idea, You can derive semantic relationships between words from the co-occurrence matrix. Given a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46df2d",
   "metadata": {},
   "source": [
    "Source Credits - https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = './raw_data/glove.twitter.27B.25d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "#change below line if computing normal stats is too slow\n",
    "embedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58799151",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1c3ea",
   "metadata": {},
   "source": [
    "Some Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff331c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8666a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "embed_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ad3130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Neural Network\n",
    "model = Sequential()\n",
    "\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length=maxlen, trainable=False))\n",
    "\n",
    "#LSTM \n",
    "model.add(LSTM(units=128 , return_sequences = True , recurrent_dropout = 0.25 , dropout = 0.25))\n",
    "model.add(LSTM(units=64 , recurrent_dropout = 0.1 , dropout = 0.1))\n",
    "model.add(Dense(units = 32 , activation = 'relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62986fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f057dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs , callbacks = [learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6eb350",
   "metadata": {},
   "source": [
    "# Analyse after training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc02ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of the model on Training Data is - \" , model.evaluate(x_train,y_train)[1]*100 , \"%\")\n",
    "print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(X_test,y_test)[1]*100 , \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(10)]\n",
    "fig , ax = plt.subplots(1,2)\n",
    "train_acc = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_loss = history.history['val_loss']\n",
    "fig.set_size_inches(20,10)\n",
    "\n",
    "ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n",
    "ax[0].plot(epochs , val_acc , 'ro-' , label = 'Testing Accuracy')\n",
    "ax[0].set_title('Training & Testing Accuracy')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax[1].plot(epochs , train_loss , 'go-' , label = 'Training Loss')\n",
    "ax[1].plot(epochs , val_loss , 'ro-' , label = 'Testing Loss')\n",
    "ax[1].set_title('Training & Testing Loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c438f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, pred, target_names = ['Fake','Not Fake']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7673a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903bf302",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(cm , index = ['Fake','Original'] , columns = ['Fake','Original'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a10458",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Fake','Original'] , yticklabels = ['Fake','Original'])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
