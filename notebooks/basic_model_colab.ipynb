{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d03449",
   "metadata": {
    "id": "c8d03449"
   },
   "source": [
    "# Load data\n",
    "### Link : https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?resource=download&select=True.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "BLlAQZSLTI57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BLlAQZSLTI57",
    "outputId": "98886850-c954-43c2-dacb-dfaf01679599"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25150/1408506528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2866494",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2866494",
    "outputId": "4c5d8a08-15d9-4e00-a6a4-a3504bc448a4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn import set_config; set_config(display='diagram')\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.model_selection import train_test_split \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef33c15",
   "metadata": {
    "id": "eef33c15"
   },
   "outputs": [],
   "source": [
    "data_path = '/content/drive/My Drive/fake_data/'\n",
    "\n",
    "fake = pd.read_csv(data_path +'Fake.csv')\n",
    "true = pd.read_csv(data_path +'True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b3fba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8a0b3fba",
    "outputId": "18344964-fad0-428c-d505-063fafa5f603"
   },
   "outputs": [],
   "source": [
    "fake.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a96f9",
   "metadata": {
    "id": "bb9a96f9"
   },
   "source": [
    "# Preparing the data for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed2ae8",
   "metadata": {
    "id": "98ed2ae8"
   },
   "outputs": [],
   "source": [
    "# Creating True columns for both dataframes 1 for true and 0 for fake\n",
    "\n",
    "true['true'] = 1\n",
    "fake['true'] = 0\n",
    "\n",
    "# Concatenate the two in one dataframe\n",
    "\n",
    "data = pd.concat([fake, true])\n",
    "\n",
    "# Reset the index\n",
    "data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69687eca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69687eca",
    "outputId": "34755d7e-69b5-498e-b955-69c588a29537"
   },
   "outputs": [],
   "source": [
    "# The data is balanced\n",
    "\n",
    "data.true.value_counts() / len(data) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a983b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "96a983b2",
    "outputId": "effc35a9-762d-4d93-cdf3-147112529cb6"
   },
   "outputs": [],
   "source": [
    "# Check true fake news along each subject\n",
    "# From the chart we see that subject column isn't important so i'll drop it\n",
    "\n",
    "sns.countplot(x = 'subject', hue = 'true', data = data)\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.legend(loc = 'upper center');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f9080",
   "metadata": {
    "id": "1f2f9080"
   },
   "outputs": [],
   "source": [
    "# Merging text and title columns in one\n",
    "\n",
    "data['text'] = data['title'] + \" \" + data['text']\n",
    "\n",
    "# Creating a dataframe of text and true columns only (So after this point df is our dataframe)\n",
    "df = data.loc[:, ['text', 'true']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d63b1b",
   "metadata": {
    "id": "f2d63b1b"
   },
   "source": [
    "### Now, we have a dataframe that contains one feature 'text' and the target 'true', the next step is to clean the text column by removing puctuations, making all letters lower, removing digits, and strip from extra space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21f505",
   "metadata": {
    "id": "bc21f505"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b7212",
   "metadata": {
    "id": "099b7212"
   },
   "outputs": [],
   "source": [
    "def cleaning(sentence):\n",
    "    \n",
    "    # making all letters lower_case\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Removing numbers\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "    \n",
    "    # Removing punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') \n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# Apply the function on the dataframe using pd.map\n",
    "\n",
    "df['text'] = df['text'].map(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Wn5JrqKXk2t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Wn5JrqKXk2t",
    "outputId": "264b467b-b53f-4327-f0c1-f55d66135841"
   },
   "outputs": [],
   "source": [
    "# Only for Colab, remove after\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1930f",
   "metadata": {
    "id": "e4f1930f"
   },
   "outputs": [],
   "source": [
    "# Tokenize texts and remove stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def prep(sentence) :\n",
    "    sentence = word_tokenize(sentence)\n",
    "    sentence = [w for w in sentence if not w in stop_words]\n",
    "    return sentence\n",
    "\n",
    "df['text'] = df['text'].map(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa7092",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "ceaa7092",
    "outputId": "6b60544c-87e9-4eca-9699-07708cb8fdb2"
   },
   "outputs": [],
   "source": [
    "# Take a look at the length of each text\n",
    "# Here for evey row i'm getting its length(how many words it contains)\n",
    "\n",
    "sns.histplot(df['text'].map(lambda x : len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62c5a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "2f62c5a6",
    "outputId": "4904b032-d97e-4005-9a36-b99e1d315bac"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df['text'].map(lambda x : len(x)))\n",
    "plt.xticks([i*500 for i in range(10)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf17c1",
   "metadata": {
    "id": "a1cf17c1"
   },
   "source": [
    "## We can see that most of the data has less than 500 words, so to avoid large and useless padding i'll remove rows that contain more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae565ff5",
   "metadata": {
    "id": "ae565ff5"
   },
   "outputs": [],
   "source": [
    "# Create a column for the number of words in each row\n",
    "\n",
    "df['num_words'] = df['text'].map(lambda x : len(x))\n",
    "\n",
    "# Number of rows with more than 500 words is 3125\n",
    "\n",
    "len(df[df['num_words'] > 500])\n",
    "\n",
    "# Getting rid of rows that have more than 1000 words\n",
    "\n",
    "df = df[df['num_words'] <= 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9db98",
   "metadata": {
    "id": "efd9db98"
   },
   "outputs": [],
   "source": [
    "# Dropping the num_words columns because i just used it to filter out data with large number of words\n",
    "\n",
    "del df['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9941f",
   "metadata": {
    "id": "ebc9941f"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25657840",
   "metadata": {
    "id": "25657840"
   },
   "source": [
    " Up to this moment, we cleaned the data, so we have one feature 'text' which is a list of words and the target.\n",
    "Now, i'll split the data into train and test, then i'll fit a tokenizer on the training set and transform training and testing sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606a5ea",
   "metadata": {
    "id": "2606a5ea"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686fd1c8",
   "metadata": {
    "id": "686fd1c8"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "\n",
    "X = df['text']             \n",
    "y = df['true'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26012045",
   "metadata": {
    "id": "26012045"
   },
   "outputs": [],
   "source": [
    "# Initializing the tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# The tokenization learns a dictionary that maps a token (integer) to each word\n",
    "# It can be done only on the train set - we are not supposed to know the test set!\n",
    "# This tokenization also lowercases your words, apply some filters, and so on - you can check the doc if you want\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a0355",
   "metadata": {
    "id": "238a0355"
   },
   "outputs": [],
   "source": [
    "# Calculating the number of different words in the training set\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a85430",
   "metadata": {
    "id": "53a85430"
   },
   "outputs": [],
   "source": [
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad3171",
   "metadata": {
    "id": "9cad3171"
   },
   "source": [
    "## Padding to make the input of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb7df3d",
   "metadata": {
    "id": "1eb7df3d"
   },
   "outputs": [],
   "source": [
    "X_tr = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "\n",
    "X_te = pad_sequences(X_test_token, dtype='float32', padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef963dbb",
   "metadata": {
    "id": "ef963dbb"
   },
   "source": [
    "# Creating our model\n",
    "\n",
    "- Embedding layer whose input_dim is the size of your vocabulary + 1 to consider 0 that is added by padding, and whose output_dim is the size of the     embedding space you want to have\n",
    "- RNN (SimpleRNN, LSTM, GRU) layer\n",
    "- Dense layer\n",
    "- Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ed511",
   "metadata": {
    "id": "e90ed511"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "    input_dim=vocab_size+1,\n",
    "    output_dim= 30,\n",
    "    mask_zero=True, ),\n",
    "    layers.LSTM(20),\n",
    "    layers.Dense(10, activation = 'relu'),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "    \n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd5ef3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54cd5ef3",
    "outputId": "44a26e06-c1e3-4368-ab74-31efb52a581d"
   },
   "outputs": [],
   "source": [
    "# Early stopping and train the model\n",
    "\n",
    "es = EarlyStopping(patience = 4)\n",
    "\n",
    "model, history = model.fit(X_tr, y_train, callbacks = [es], epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09e230",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3d09e230",
    "outputId": "25ac6731-2f39-475b-edb3-f4471bdd0259"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_te, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2pnTR6e7a5DI",
   "metadata": {
    "id": "2pnTR6e7a5DI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "f2d63b1b"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
