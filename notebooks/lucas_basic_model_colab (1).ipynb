{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d03449",
   "metadata": {
    "id": "c8d03449"
   },
   "source": [
    "# Load data\n",
    "### Link : https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?resource=download&select=True.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "BLlAQZSLTI57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1854,
     "status": "ok",
     "timestamp": 1662542704663,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "BLlAQZSLTI57",
    "outputId": "70270a89-c237-443b-925c-221e7bf62421"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2866494",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6004,
     "status": "ok",
     "timestamp": 1662542710662,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "d2866494",
    "outputId": "af8ba700-4406-4cea-cb9e-edb82e81b0cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lucaspicot/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\n",
    "from sklearn import set_config; set_config(display='diagram')\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import string\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.model_selection import train_test_split \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef33c15",
   "metadata": {
    "executionInfo": {
     "elapsed": 3686,
     "status": "ok",
     "timestamp": 1662542714338,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "eef33c15"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Fake.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cl/x784q0v50m5417cgrr7ykdzw0000gn/T/ipykernel_85096/504633907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Fake.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./True.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/envs/lewagon/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Fake.csv'"
     ]
    }
   ],
   "source": [
    "fake = pd.read_csv('./Fake.csv')\n",
    "true = pd.read_csv('./True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b3fba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1662542714338,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "8a0b3fba",
    "outputId": "3748b780-2802-4551-b8b3-f74eed3fc8da"
   },
   "outputs": [],
   "source": [
    "fake.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a96f9",
   "metadata": {
    "id": "bb9a96f9"
   },
   "source": [
    "# Preparing the data for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed2ae8",
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1662542714339,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "98ed2ae8"
   },
   "outputs": [],
   "source": [
    "# Creating True columns for both dataframes 1 for true and 0 for fake\n",
    "\n",
    "true['true'] = 1\n",
    "fake['true'] = 0\n",
    "\n",
    "# Concatenate the two in one dataframe\n",
    "\n",
    "data = pd.concat([fake, true])\n",
    "\n",
    "# Reset the index\n",
    "data.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69687eca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1662542714339,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "69687eca",
    "outputId": "a824e266-9427-4d18-8a7d-9be33e09e014"
   },
   "outputs": [],
   "source": [
    "# The data is balanced\n",
    "\n",
    "data.true.value_counts() / len(data) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a983b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1662542715356,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "96a983b2",
    "outputId": "161a02ee-a313-4d7d-e6cf-bedfa0545eee"
   },
   "outputs": [],
   "source": [
    "# Check true fake news along each subject\n",
    "# From the chart we see that subject column isn't important so i'll drop it\n",
    "\n",
    "sns.countplot(x = 'subject', hue = 'true', data = data)\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.legend(loc = 'upper center');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f9080",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662542715356,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "1f2f9080"
   },
   "outputs": [],
   "source": [
    "# Merging text and title columns in one\n",
    "\n",
    "data['text'] = data['title'] + \" \" + data['text']\n",
    "\n",
    "# Creating a dataframe of text and true columns only (So after this point df is our dataframe)\n",
    "df = data.loc[:, ['text', 'true']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d63b1b",
   "metadata": {
    "id": "f2d63b1b"
   },
   "source": [
    "### Now, we have a dataframe that contains one feature 'text' and the target 'true', the next step is to clean the text column by removing puctuations, making all letters lower, removing digits, and strip from extra space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21f505",
   "metadata": {
    "id": "bc21f505"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b7212",
   "metadata": {
    "executionInfo": {
     "elapsed": 24619,
     "status": "ok",
     "timestamp": 1662542739973,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "099b7212"
   },
   "outputs": [],
   "source": [
    "def cleaning(sentence):\n",
    "    \n",
    "    # making all letters lower_case\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Removing numbers\n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit())\n",
    "    \n",
    "    # Removing punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') \n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# Apply the function on the dataframe using pd.map\n",
    "\n",
    "df['text'] = df['text'].map(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Wn5JrqKXk2t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1662542740520,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "2Wn5JrqKXk2t",
    "outputId": "a0f2c0b9-a29b-42fa-b589-dcf6114ea985"
   },
   "outputs": [],
   "source": [
    "# Only for Colab, remove after\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1930f",
   "metadata": {
    "executionInfo": {
     "elapsed": 68197,
     "status": "ok",
     "timestamp": 1662542808715,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "e4f1930f"
   },
   "outputs": [],
   "source": [
    "# Tokenize texts and remove stop words\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def prep(sentence) :\n",
    "    sentence = word_tokenize(sentence)\n",
    "    sentence = [w for w in sentence if not w in stop_words]\n",
    "    return sentence\n",
    "\n",
    "df['text'] = df['text'].map(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaa7092",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 1076,
     "status": "ok",
     "timestamp": 1662542809772,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "ceaa7092",
    "outputId": "d9f2bf29-0911-4e77-df86-ed9f3523e6b5"
   },
   "outputs": [],
   "source": [
    "# Take a look at the length of each text\n",
    "# Here for evey row i'm getting its length(how many words it contains)\n",
    "\n",
    "sns.histplot(df['text'].map(lambda x : len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62c5a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1662542809772,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "2f62c5a6",
    "outputId": "577d8164-3ac7-4526-8fda-0b5e2a227fad"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df['text'].map(lambda x : len(x)))\n",
    "plt.xticks([i*500 for i in range(10)]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf17c1",
   "metadata": {
    "id": "a1cf17c1"
   },
   "source": [
    "## We can see that most of the data has less than 500 words, so to avoid large and useless padding i'll remove rows that contain more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae565ff5",
   "metadata": {
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1662542810105,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "ae565ff5"
   },
   "outputs": [],
   "source": [
    "# Create a column for the number of words in each row\n",
    "\n",
    "df['num_words'] = df['text'].map(lambda x : len(x))\n",
    "\n",
    "# Number of rows with more than 500 words is 3125\n",
    "\n",
    "len(df[df['num_words'] > 500])\n",
    "\n",
    "# Getting rid of rows that have more than 1000 words\n",
    "\n",
    "df = df[df['num_words'] <= 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd9db98",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1662542810106,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "efd9db98"
   },
   "outputs": [],
   "source": [
    "# Dropping the num_words columns because i just used it to filter out data with large number of words\n",
    "\n",
    "del df['num_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc9941f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1662542810106,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "ebc9941f",
    "outputId": "3808e8e7-09a3-4a87-c0be-3ece2925be3e"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25657840",
   "metadata": {
    "id": "25657840"
   },
   "source": [
    " Up to this moment, we cleaned the data, so we have one feature 'text' which is a list of words and the target.\n",
    "Now, i'll split the data into train and test, then i'll fit a tokenizer on the training set and transform training and testing sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606a5ea",
   "metadata": {
    "id": "2606a5ea"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686fd1c8",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1662542810106,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "686fd1c8"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "\n",
    "X = df['text']             \n",
    "y = df['true'] \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26012045",
   "metadata": {
    "executionInfo": {
     "elapsed": 10814,
     "status": "ok",
     "timestamp": 1662542820917,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "26012045"
   },
   "outputs": [],
   "source": [
    "# Initializing the tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# The tokenization learns a dictionary that maps a token (integer) to each word\n",
    "# It can be done only on the train set - we are not supposed to know the test set!\n",
    "# This tokenization also lowercases your words, apply some filters, and so on - you can check the doc if you want\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# We apply the tokenization to the train and test set\n",
    "\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a0355",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1662542820918,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "238a0355"
   },
   "outputs": [],
   "source": [
    "# Calculating the number of different words in the training set\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a85430",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1662542820918,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "53a85430",
    "outputId": "fa8f6bc8-9558-4945-97a6-e751070661d7"
   },
   "outputs": [],
   "source": [
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cad3171",
   "metadata": {
    "id": "9cad3171"
   },
   "source": [
    "## Padding to make the input of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb7df3d",
   "metadata": {
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1662542821627,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "1eb7df3d"
   },
   "outputs": [],
   "source": [
    "X_tr = pad_sequences(X_train_token, dtype='float32', padding='post', maxlen = 185)\n",
    "\n",
    "X_te = pad_sequences(X_test_token, dtype='float32', padding='post', maxlen = 185)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H1N_5x22dx7y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662542821627,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "H1N_5x22dx7y",
    "outputId": "88a71241-c53b-4c23-86a4-f185ba9e1568"
   },
   "outputs": [],
   "source": [
    "X_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef963dbb",
   "metadata": {
    "id": "ef963dbb"
   },
   "source": [
    "# Creating our model\n",
    "\n",
    "- Embedding layer whose input_dim is the size of your vocabulary + 1 to consider 0 that is added by padding, and whose output_dim is the size of the     embedding space you want to have\n",
    "- RNN (SimpleRNN, LSTM, GRU) layer\n",
    "- Dense layer\n",
    "- Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ed511",
   "metadata": {
    "executionInfo": {
     "elapsed": 2803,
     "status": "ok",
     "timestamp": 1662544359536,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "e90ed511"
   },
   "outputs": [],
   "source": [
    "#Zein's model \n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "reg_l1 = regularizers.L1(0.01)\n",
    "reg_l2 = regularizers.L2(0.01)\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Embedding(\n",
    "    input_dim=vocab_size+1,\n",
    "    output_dim= 30,\n",
    "    mask_zero=True, ),\n",
    "    layers.LSTM(10),\n",
    "    layers.Dense(10, activation = 'relu',kernel_regularizer = reg_l1),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.Dense(8, activation = 'relu',kernel_regularizer = reg_l1),\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.Dense(1, activation=\"sigmoid\")\n",
    "    \n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd5ef3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54cd5ef3",
    "outputId": "8b0b2439-91a1-42a2-a060-d10434e66e48"
   },
   "outputs": [],
   "source": [
    "# Early stopping and train the model\n",
    "\n",
    "es = EarlyStopping(patience = 2)\n",
    "\n",
    "model.fit(X_tr, y_train, callbacks = [es], epochs = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d09e230",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12041,
     "status": "ok",
     "timestamp": 1662544154701,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "3d09e230",
    "outputId": "7524ab52-a868-405f-8ba7-cf8a58739016",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_te, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qglWHf3syw_o",
   "metadata": {
    "id": "qglWHf3syw_o"
   },
   "source": [
    "# **Model try with a new dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2pnTR6e7a5DI",
   "metadata": {
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1662543578922,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "2pnTR6e7a5DI"
   },
   "outputs": [],
   "source": [
    "new_dataset = pd.read_csv('./Train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vMt6DOTCzkVh",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1662543578923,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "vMt6DOTCzkVh"
   },
   "outputs": [],
   "source": [
    "def changing_target(df):\n",
    "  df.loc[df[\"Labels\"] == 1, \"category\"] = int(0)\n",
    "  df.loc[df[\"Labels\"] == 5, \"category\"] = int(1)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6tyEoU9jzlTQ",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662543578923,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "6tyEoU9jzlTQ"
   },
   "outputs": [],
   "source": [
    "df_with_target = changing_target(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nDPP7gkLzujD",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662543578923,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "nDPP7gkLzujD"
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_with_target.drop(columns='Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VYBpueA8zx65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1662543579230,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "VYBpueA8zx65",
    "outputId": "2635be64-7d55-4ed0-b6e6-6c5efeb999ae"
   },
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "sns.countplot(df_cleaned.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y6V3dRmezzp5",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1662543579230,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "Y6V3dRmezzp5"
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_cleaned.dropna() # drop the labels 2,3,4,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dTzI0Eahz101",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662543579527,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "dTzI0Eahz101",
    "outputId": "92065fab-a139-4537-a4dc-de5e6b0074f1"
   },
   "outputs": [],
   "source": [
    "# The data is balanced\n",
    "\n",
    "df_cleaned.category.value_counts() / len(df_cleaned) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cgiwD9IUz4XT",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662543579527,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "cgiwD9IUz4XT"
   },
   "outputs": [],
   "source": [
    "# Merging text and title columns in one\n",
    "\n",
    "df_cleaned['Text'] = df_cleaned['Text_Tag'] + \" \" + df_cleaned['Text']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5YtCixdNz56j",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662543579527,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "5YtCixdNz56j"
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe of text and true columns only (So after this point df is our dataframe)\n",
    "df_cleaned = df_cleaned.loc[:, ['Text', 'category']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tchuFolBz_eP",
   "metadata": {
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1662543579830,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "tchuFolBz_eP"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply the function on the dataframe using pd.map\n",
    "\n",
    "df_cleaned['Text'] = df_cleaned['Text'].map(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7QtBl-Bw0Fto",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 192,
     "status": "ok",
     "timestamp": 1662543581489,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "7QtBl-Bw0Fto",
    "outputId": "530ab5c5-e7e9-4cc4-9b43-975bc07d77ba"
   },
   "outputs": [],
   "source": [
    "df_cleaned['Text'][12] ## it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6TqnU3jg0I8s",
   "metadata": {
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1662543583698,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "6TqnU3jg0I8s"
   },
   "outputs": [],
   "source": [
    "df_cleaned['Text'] = df_cleaned['Text'].map(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DQfoQNyC0MWr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1662543583698,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "DQfoQNyC0MWr",
    "outputId": "c803ddb0-bf1e-4fe2-e7d7-5013a8c96df6"
   },
   "outputs": [],
   "source": [
    "df_cleaned['Text'].map(lambda x : len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W4vIjeD20O4S",
   "metadata": {
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1662543585266,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "W4vIjeD20O4S"
   },
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "\n",
    "X_new_data = df_cleaned['Text']             \n",
    "y_new_data = df_cleaned['category'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZcbFNR3t0RNy",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662543586020,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "ZcbFNR3t0RNy"
   },
   "outputs": [],
   "source": [
    "# We apply the tokenization to the train and test set\n",
    "\n",
    "X_new_data = tokenizer.texts_to_sequences(X_new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liqvsDwy0TNn",
   "metadata": {
    "executionInfo": {
     "elapsed": 184,
     "status": "ok",
     "timestamp": 1662543587520,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "liqvsDwy0TNn"
   },
   "outputs": [],
   "source": [
    "##padding the data to a shape of 500\n",
    "\n",
    "X_new_data_padded = pad_sequences(X_new_data, dtype='float32', padding='post', maxlen = 185 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FF0lpGbH0Zsa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1662543588272,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "FF0lpGbH0Zsa",
    "outputId": "9b0eba1d-f791-42f9-d026-f8d8a53ef612"
   },
   "outputs": [],
   "source": [
    "X_new_data_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IWtvrNG10be5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5757,
     "status": "ok",
     "timestamp": 1662544165066,
     "user": {
      "displayName": "Lucas Picot",
      "userId": "06101582352497340295"
     },
     "user_tz": -120
    },
    "id": "IWtvrNG10be5",
    "outputId": "3f30b799-74c7-4277-d4f6-83caee3c246e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_new_data_padded, y_new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50fd91",
   "metadata": {},
   "source": [
    "# Merge the 2 cleaned datasets (from 'Hackathon' and from 'Fake and real news dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b622260b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_new_data_padded.shape ##data from 'Hackaton'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a8d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr.shape # train data from 'Fake and real news dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0548366",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te.shape # test data from 'Fake and real news dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a09c8a",
   "metadata": {},
   "source": [
    "*don't forget to also merge the 3 targets which are y_train, y_test, y_new_data *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8414a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape, y_test.shape, y_new_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5293d62e",
   "metadata": {},
   "source": [
    "Marco sends me a big dataset of 65 829 rows, with data and target. I have to split it between data and target y. After, I need to split between train and test. After I need to tokenize. And after I need to run the model on the new big dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7076db57",
   "metadata": {},
   "source": [
    "*the number of rows we should have in our big dataset at the end from the 3 datasets from kaggle*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f56b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_rows = 3669 + 29241 + 12532 + 20387\n",
    "nb_rows "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50dec6",
   "metadata": {},
   "source": [
    "*and try to add even more datasets because a lot of them are available on Kaggle*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "f2d63b1b"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
